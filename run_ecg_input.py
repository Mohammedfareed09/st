# run_ecg_predictions.py

import sys, os, json
import torch
import numpy as np

# ──────────────────────────────────────────────────────────────────────────────
# 1) allow importing the local fairseq_signals package
sys.path.append(os.path.join(os.getcwd(), "fairseq-signals"))
from fairseq_signals.models import build_model_from_checkpoint
# ──────────────────────────────────────────────────────────────────────────────

# ──────────────────────────────────────────────────────────────────────────────
# 2) load the finetuned ECG-FM model (12-lead checkpoint)
ckpt_path = os.path.join("ckpts", "mimic_iv_ecg_finetuned.pt")
model = build_model_from_checkpoint(ckpt_path)
model.eval()
# ──────────────────────────────────────────────────────────────────────────────

# ──────────────────────────────────────────────────────────────────────────────
# 3) YOUR ECG DATA: edit this with your fixed 1D array
data = [
  
0.68007,1.16935,-0.32256,1.20946,-0.63538,1.43404,-0.58725,0.95278,-1.18081,0.26298,-0.15412,-0.22630,0.23089,-1.56581,1.52228,-0.98830,1.43404,-0.19422,1.34581,0.39933,0.90466,1.20144,-0.66746,1.51426,0.37527,1.15331,-0.63538,1.32175,-1.29310,0.45548,-0.39475,-0.18620,0.11860,-1.99093,0.93674,-2.30375,0.22287,-1.60592,-0.32256,0.01433,0.77632,1.44207,-0.82788,1.97947,-0.21828,1.54634,-0.48298,1.20946,-0.70757,0.57580,0.43944,0.99289,1.73884,-1.37331,1.31373,-1.65405,0.77632,-1.54977,0.44746,-1.00434,0.38329,-0.66746,-2.34385,0.35121,-1.80644,1.46613,-1.53373,0.65601,-1.70217,-0.37068,-0.26641,-0.45891,-0.69152,-1.06049,1.12925,-1.96686,0.51965,-0.92413,0.91268,-1.13268,-0.07391,-0.36266,-1.98291,-0.49902,-1.80644,0.60788,-2.27166,0.32715,-1.53373,-0.62736,-0.37870,-0.16214,0.37527,-1.01236,1.18539,-1.69415,1.27362,-0.60329,0.43142,-0.97226,0.47152,-0.09797,-0.13807,0.55173,-1.54977,0.84851,-2.03905,0.60788,-1.65405,-0.41881,-0.89205,-0.73163,-0.45891,-2.54438,0.52767,-2.17541,0.76830,-0.70757,-0.33058,-1.15674,-0.25839,-0.33860,-1.58988,-0.36266,-1.14070,1.16935,-1.68613,1.40196,-0.21026,-0.45891,-0.88403,-1.35727,-0.58725,-1.71019,0.42340,-1.82249,1.08112,-0.88403,0.80841,-0.72361,0.51965,0.75226,-0.49100,0.37527,-1.81447,0.90466,-0.71559,0.47954,-0.66746,-0.14609,0.58382,0.15068,0.55975,-1.02841,1.65863,-0.85996,1.25758,-0.78778,0.47954,0.10256,1.16935,1.45811,-0.36266,1.41800,-0.86799,1.67468,-0.28245,1.16935,-0.77975,-0.36266,0.05443,-0.65142,0.81643,-0.65142,1.29769,-0.57923,2.19604,-0.83590,0.99289,-0.36266,0.29506,-0.08995,-1.57384,0.61590,-0.77173,1.68270,-0.62736,1.53832,-1.26904,-0.21026,-0.27443,-0.11401,0.91268,-0.39475,1.24154,-0.98830,2.12385,-1.28508,0.61590,-0.57923,0.39933,0.80841,-1.37331,-0.06588,-1.59790,0.58382,-1.67009,0.76830,-0.77975,-0.62736,-0.18620,-0.62736,0.05443,-1.34123,0.87257,-1.82249,0.91268,-0.36266,0.58382,-0.87601,0.78434,0.19079,-2.03103,-0.01776,-1.54175,0.57580,-1.88665,0.22287,-1.60592,-0.18620,0.27902,-0.44287,0.71215,-0.75569,2.17198,-0.86799,1.82707,-0.29849,0.07047,-0.67548,0.14266,1.11320,-0.66746,1.00893,-1.79040,0.76830,-1.38133,0.80038,-0.93215,0.00630,0.23089,-0.27443,0.65601,-0.28245,2.47678,-0.63538,1.08112,-0.67548,0.15068,-0.29849,0.39131,0.31110,-1.72623,-0.18620,-1.72623,0.63996,-0.25037,0.55173,-1.53373,-0.63538,-0.18620,-1.73426,-0.50704,-1.13268,0.65601,-1.64602,0.11860,-1.61394,-0.62736,-0.51506,-0.71559,-0.25839,-2.03103,-0.62736,-2.14333,0.63996,-1.15674,0.80038,-0.35464,-0.04984,0.91268,-0.04182,1.16935,-1.53373,0.92070,-1.54977,1.46613,0.03037,0.42340,0.19881,0.45548,1.54634,-0.53912,1.46613,-0.31454,1.09716,-0.84392,0.51965,-0.88403,-0.50704,0.48757,-0.93215,0.23089,-1.53373,0.08651,-1.81447,0.35923,-0.37068,-0.45891,-0.60329,-0.16214,0.30308,-1.87061,0.14266,-1.89468,-0.11401,-1.65405,-0.26641,-2.06312,-0.87601,0.47954,-0.70757,0.06245,-1.74228,0.80038,-1.31716,0.73622,-0.00172,0.15870,-0.17016,0.40736,0.84049,-1.09257,1.17737,-0.99632,1.00091,-0.63538,0.83247,-0.36266,-0.22630,1.01695,-0.69954,0.69611,-0.71559,1.28967,-1.06049,0.98487,-0.64340,-0.00974,-0.41881,-0.59527,0.77632,-1.30914,0.98487,-1.79040,0.76830,-0.21828,0.93674,-0.49902,-0.76371,0.27902,-0.90809,0.82445,-1.53373,0.99289,-1.18883,0.65601,-0.37068,-0.55517,-0.08995,-0.15412,0.66403,-1.12466,1.50623,-1.53373,1.57842,-0.02578,0.18277,-1.16476,-1.01236,-0.06588,-1.53373,0.22287,-1.74228,0.61590,-1.67009,0.65601,-0.58725,-0.37068,-0.01776,-0.75569,1.24154,-1.53373,0.48757,-1.06049,0.73622,-0.89205,-0.15412,-1.32518,-0.61933,1.37790,-0.52308,0.72820,-1.13268,1.85114,-0.63538,1.40196,-0.37068,-0.45891,0.12662,-0.13807,1.12925,-0.88403,1.15331,-0.86799,0.44746,-0.79580,0.21485,-0.98830,-1.39737,-0.03380,-1.38935,0.59986,-1.38935,1.25758,-0.39475,1.62655,0.40736,0.07047,0.51965,-0.06588,1.70676,-0.29849,1.53030,-0.61131,1.20946,-0.00172,0.26298,-0.28245,-0.97226,1.15331,-0.58725,1.04904,-0.62736,1.36988,-1.07653,1.33779,-0.66746,-0.75569,-0.29849,-1.01236,0.90466,-1.53373,0.18277,-1.67811,0.40736,-0.37068,0.31110,-0.72361,-0.64340,0.56778,-1.53373,0.77632,-1.04445,0.92070,-0.48298,1.57040,0.52767,0.24693,0.06245,-0.25839,1.54634,-0.60329,0.90466,-1.35727,0.87257,0.14266,0.67205,0.66403,0.14266,1.85114,0.75226,1.85114,-0.64340,1.19341,-0.76371,0.93674,0.23089,0.14266,0.55173,0.19079,1.41800,-0.88403,1.60249,-1.00434,0.23089,-0.92413,-0.26641,-0.73163,-1.02039,0.93674,-1.70217,0.15068,-1.91072,0.60788,-0.74767,1.19341,0.68809,0.04641,0.43142,-0.06588,0.91268,-1.53373,1.46613,-1.14070,0.61590,-0.58725,0.39131,-0.44287,-1.14872,0.51163,-1.10060,0.84049,-0.85996,0.70413,-1.53373,0.13464,-0.37068,-1.18883,-0.05786,-1.18081,0.55975,-1.31716,0.91268,-2.00697,0.74424,-0.37068,0.01433,-0.66746,-1.00434,0.47954,-1.67811,0.56778,-1.53373,0.43944,-1.00434,0.57580,-0.45089,-0.96424,-0.26641,-1.75832,0.49559,-1.77436,0.74424,-1.99093,0.26298,0.14266,0.86455,0.00630,-0.38673,1.33779,-1.53373,1.20946,-1.25299,1.19341,-0.49100,1.69874,0.84049,-0.35464,0.50361,-0.03380,1.06508,-1.32518,1.14529,-1.23695,0.67205,-0.12203,0.65601,0.22287,-0.46694,1.83510,-0.62736,1.64259,-0.67548,0.96883,-0.33860,1.52228,0.39933,0.26298,0.65601,-0.41079,0.61590,-1.30112,1.04904,-1.35727,0.71215,-0.31454,0.07047,-0.43485,-0.51506,0.93674,-1.25299,0.90466,-1.09257,1.04904,-0.22630,1.04102,0.92872,-0.07391,1.12123,-0.19422,1.52228,-0.85996,1.41800,-0.88403,1.16935,0.76028,0.67205,0.46350,-0.51506,1.19341,-0.77173,1.41800,-0.25839,0.94476,-1.05247,0.77632,0.22287,-0.41079,0.79236,-1.02841,0.87257,-1.37331,1.68270,-0.12203,1.09716,0.66403,0.54371,0.22287,-0.47496,1.70676,-0.54715,1.89124,-0.42683,1.32175,-0.25037,1.59447,1.00893,-0.14609,1.18539,-0.12203,1.63457,-0.96424,1.47415,-0.78778,0.52767,0.40736,0.56778,1.03299,-0.35464,1.75489,-0.42683,1.64259,-0.16214,1.65863,0.65601,1.83510,1.08112,0.19079,1.92333,0.51163,2.26021,-0.44287,2.09979,-0.41881,0.95278,0.52767,-0.07391,0.64799,-0.65142,1.60249,-0.21828,1.70676,-0.29849,0.95278,-0.57121,0.27100,0.42340,-0.96424,1.38592,0.00630,1.15331,-1.27706,1.22550,-1.25299,-0.25037,-0.15412,-0.65142,-0.37068,-1.92676,0.14266,-2.54438,0.23089,-0.77173,0.55173,-0.56319,0.03839,0.11860,-1.77436,0.08651,-1.13268,1.53832,-0.77975,0.91268,-1.39737,0.73622,0.68007,0.32715,0.03037,-1.31716,1.42602,-1.21289,1.39394,-1.04445,0.06245,0.14266,1.55436,0.93674,-0.53912,0.80038,-0.68350,1.48217,-0.66746,1.18539,-1.73426,0.15870,0.39131,-0.48298,0.39131,-0.45089,1.80301,-0.97226,1.25758,-0.73965,0.00630,-0.87601,0.12662,-0.13005,-1.69415,0.53569,-1.07653,0.68007,-1.53373,0.27100,-1.67009,-0.21828,0.09454,-0.87601,0.10256,-1.83051,0.39131,-1.74228,0.53569,-0.90809,0.39933,-0.39475,0.67205,0.42340,-1.01236,0.76830,-0.89205,0.91268,-0.88403,1.65863,-0.79580,0.16672,0.03037,-0.54715,0.40736,-1.14070,1.09716,-1.14872,0.87257,0.07047,0.96883,-0.50704,0.02235,0.69611,-0.70757,0.77632,-0.92413,0.77632,-1.53373,0.71215,-0.84392,0.11860,0.50361,0.11860,1.11320,-0.87601,1.63457,-0.06588,1.32175,0.11058,1.05706,0.16672,0.89664,1.40196,0.34319,1.81103,0.06245,1.99552,0.35923,1.22550,-0.95622,0.37527,1.68270,0.56778,0.91268,-1.02039,1.70676,-0.24235,1.77093,-0.50704,0.87257,0.11860,0.11058,1.05706,-0.36266,1.35383,-0.62736,1.55436,0.15068,1.58644,-0.17016,0.25496,0.79236,0.71215,1.93135,0.59986,2.41261,0.72820,3.03825,0.41538,0.88060,0.83247,0.85653,0.41538,-1.38935,0.95278,-1.24497,0.45548,-2.11124,-0.00974,-0.62736,-1.22091,0.19881,-0.03380,-0.08193,-1.62196,1.07310,-2.23958,0.37527,-1.53373,-0.85996,-0.60329,-0.05786,-0.43485,-1.65405,0.59986,-1.09257,1.45009,0.51965,2.01958,-0.11401,1.04904,1.42602,0.36725,0.82445,0.11860,2.04364,-0.53912,1.38592,0.35923,0.91268,0.65601,0.39933,1.93937,0.30308,1.42602,-0.37068,0.90466,-0.80382,0.71215,-1.06049,-0.11401,1.16133,0.06245,1.16935,-1.26904,0.71215,-0.18620,1.85114,0.39933,1.25758,0.52767,0.50361,0.77632,-1.21289,0.48757,-1.35727,0.67205,-1.53373,0.14266,-0.95622,-0.46694,0.01433,-1.65405,-0.13005,-1.53373,0.50361,-1.29310,0.22287,-0.91611,-0.34662,-0.81986,-0.00974,0.00630,-1.82249,-0.08995,-1.85457,0.42340,-1.01236,0.51163,-0.49902,0.07047,1.16935,0.02235,0.92070,-1.26102,1.03299,-0.73965,1.11320,-0.57923,-0.11401,0.08651,0.06245,0.52767,-0.97226,1.03299,-1.23695,0.68007,-0.55517,0.30308,-1.30112,-0.31454,0.39933,-1.00434,0.30308,-1.21289,0.91268,-0.80382,1.44207,0.13464,0.23891,0.14266,0.10256,0.49559,-1.08455,0.65601,-1.21289,1.34581,0.29506,1.41800,0.41538,-0.00974,0.42340,-0.57121,0.52767,-1.13268,0.96081,-0.57923,1.15331,0.51965,0.50361,0.44746,0.05443,1.50623,-0.33058,1.43404]

# ──────────────────────────────────────────────────────────────────────────────

# ──────────────────────────────────────────────────────────────────────────────
# 4) preprocess → single-lead to 12-lead
signal = np.array(data, dtype=np.float32)
x = torch.from_numpy(signal).unsqueeze(0).unsqueeze(0)  # (1,1,L)
x = x.repeat(1, 12, 1)                                   # (1,12,L)
# ──────────────────────────────────────────────────────────────────────────────

# ──────────────────────────────────────────────────────────────────────────────
# 5) run inference
with torch.no_grad():
    out = model(source=x)
    probs = torch.sigmoid(out["out"])[0].tolist()      # list of 17 floats
# ──────────────────────────────────────────────────────────────────────────────

# ──────────────────────────────────────────────────────────────────────────────
# 6) CLASS NAMES: replace these with your model’s actual 17 labels in order
label_names = [
    "Myocardial Infarction",
    "Atrial Fibrillation",
    "Bundle Branch Block",
    "Class_3", "Class_4", "Class_5", "Class_6",
    "Class_7", "Class_8", "Class_9", "Class_10",
    "Class_11", "Class_12", "Class_13", "Class_14",
    "Class_15", "Class_16"
]
assert len(label_names) == len(probs), "Must have one name per class"
# ──────────────────────────────────────────────────────────────────────────────

# ──────────────────────────────────────────────────────────────────────────────
# 7) human-friendly interpretation
def interpret(p):
    if p < 0.05:
        return "impossible"
    elif p < 0.2:
        return "good"
    elif p < 0.5:
        return "okay"
    elif p < 0.8:
        return "monitor"
    else:
        return "dangerous"
# ──────────────────────────────────────────────────────────────────────────────

# ──────────────────────────────────────────────────────────────────────────────
# 8) assemble results
results = []
for name, p in zip(label_names, probs):
    results.append({
        "class": name,
        "probability": round(p, 3),
        "interpretation": interpret(p),
    })

# 9) output JSON
print(json.dumps({"predictions": results}, indent=2))
# ──────────────────────────────────────────────────────────────────────────────

#10)ploting
# ──────────────────────────────────────────────────────────────────────────────

# import matplotlib.pyplot as plt

# plt.figure(figsize=(10, 4))
# plt.plot(signal, color='blue', linewidth=1)
# plt.title("ECG Signal")
# plt.xlabel("Time (samples)")
# plt.ylabel("Amplitude (mV)")
# plt.grid(True)
# plt.tight_layout()
# plt.show()

# ──────────────────────────────────────────────────────────────────────────────
